{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "# import spacy\n",
    "import re\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from textblob import TextBlob\n",
    "# from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split # Import the necessary function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 31308\n"
     ]
    }
   ],
   "source": [
    "imdb_data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Original row count\n",
    "original_row_count = len(imdb_data)\n",
    "\n",
    "# Filter out rows with irrelevant or neutral sentiments\n",
    "filtered_data = imdb_data[~imdb_data[\"sentiment\"].isin([\"Irrelevant\", \"Neutral\"])]\n",
    "\n",
    "# Filtered row count\n",
    "filtered_row_count = len(filtered_data)\n",
    "\n",
    "# Calculate and display the number of rows removed\n",
    "rows_removed = original_row_count - filtered_row_count\n",
    "print(f\"Rows removed: {rows_removed}\")\n",
    "\n",
    "# Assign sentiments and reviews from the filtered data\n",
    "sentiments = filtered_data[\"sentiment\"]\n",
    "reviews = filtered_data[\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14388\\3594287.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Define functions\n",
    "def remove_html(text):\n",
    "    \"\"\"Remove HTML tags from the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_brackets(text):\n",
    "    \"\"\"Remove content inside brackets from the text.\"\"\"\n",
    "    return re.sub(r'\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"Remove special characters and punctuation.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize the text into words.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    \"\"\"Remove stopwords from the text.\"\"\"\n",
    "    tokens = tokenize_text(text)\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Apply stemming to the text.\"\"\"\n",
    "    tokens = tokenize_text(text)\n",
    "    return ' '.join([ps.stem(word) for word in tokens])\n",
    "\n",
    "# Preprocess reviews\n",
    "def preprocess_review(text):\n",
    "    \"\"\"Apply all preprocessing steps to the text.\"\"\"\n",
    "    text = remove_html(text)\n",
    "    text = remove_brackets(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_text(text)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the reviews\n",
    "reviews = reviews.fillna(\"\")  # Replace NaN values with an empty string\n",
    "reviews = reviews.apply(preprocess_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(\n",
    "    reviews, sentiments, test_size=0.2, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0.0,max_df=1.0,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (84036, 7908551)\n",
      "BOW_cv_test: (9338, 7908551)\n"
     ]
    }
   ],
   "source": [
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (84036, 7908551)\n",
      "Tfidf_test: (9338, 7908551)\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0.0,max_df=1.0,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93374, 1)\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(sentiments)\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive' 'positive' ... 'positive' 'positive' 'negative']\n",
      "['negative' 'positive' 'positive' ... 'positive' 'positive' 'positive']\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow_score : 0.8620615796519411\n",
      "lr_tfidf_score : 0.8957965194109773\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
    "print(\"lr_bow_score :\",lr_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\",lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.87      0.86      0.86      9529\n",
      "    Negative       0.85      0.87      0.86      9146\n",
      "\n",
      "    accuracy                           0.86     18675\n",
      "   macro avg       0.86      0.86      0.86     18675\n",
      "weighted avg       0.86      0.86      0.86     18675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.91      0.90      9529\n",
      "    Negative       0.90      0.88      0.89      9146\n",
      "\n",
      "    accuracy                           0.90     18675\n",
      "   macro avg       0.90      0.90      0.90     18675\n",
      "weighted avg       0.90      0.90      0.90     18675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(lr_tfidf_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(max_iter=500, random_state=42)\n",
      "SGDClassifier(max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "#training the linear svm\n",
    "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
    "#fitting the svm for bag of words\n",
    "svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n",
    "print(svm_bow)\n",
    "#fitting the svm for tfidf features\n",
    "svm_tfidf=svm.fit(tv_train_reviews,train_sentiments)\n",
    "print(svm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models and vectorizers have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the Logistic Regression model trained on Bag of Words\n",
    "with open('SVM_bow.pkl', 'wb') as file:\n",
    "    pickle.dump(lr_bow, file)\n",
    "\n",
    "# Save the Logistic Regression model trained on TF-IDF features\n",
    "with open('SVM_tfidf.pkl', 'wb') as file:\n",
    "    pickle.dump(lr_tfidf, file)\n",
    "\n",
    "# Save the CountVectorizer used for Bag of Words\n",
    "with open('count_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(cv, file)\n",
    "\n",
    "# Save the TfidfVectorizer used for TF-IDF features\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tv, file)\n",
    "\n",
    "# Save the LabelBinarizer used for sentiment labels\n",
    "with open('label_binarizer.pkl', 'wb') as file:\n",
    "    pickle.dump(lb, file)\n",
    "\n",
    "print(\"\\nModels and vectorizers have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (BOW): ['positive']\n",
      "Prediction (TF-IDF): ['positive']\n"
     ]
    }
   ],
   "source": [
    "# Load models and vectorizers\n",
    "with open('SVM_bow.pkl', 'rb') as file:\n",
    "    lr_bow = pickle.load(file)\n",
    "\n",
    "with open('SVM_tfidf.pkl', 'rb') as file:\n",
    "    lr_tfidf = pickle.load(file)\n",
    "\n",
    "with open('count_vectorizer.pkl', 'rb') as file:\n",
    "    cv = pickle.load(file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    tv = pickle.load(file)\n",
    "\n",
    "with open('label_binarizer.pkl', 'rb') as file:\n",
    "    lb = pickle.load(file)\n",
    "\n",
    "# # Example usage\n",
    "def preprocess_input(text):\n",
    "    # Apply the same preprocessing steps\n",
    "    text = remove_html(text)\n",
    "    text = remove_brackets(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_text(text)\n",
    "    return text\n",
    "\n",
    "# Sample input\n",
    "sample_review = \"This movie was NOT GOOD!\"\n",
    "preprocessed_review = preprocess_input(sample_review)\n",
    "\n",
    "# Transform input using vectorizers\n",
    "bow_features = cv.transform([preprocessed_review])\n",
    "tfidf_features = tv.transform([preprocessed_review])\n",
    "\n",
    "# Predict sentiment using models\n",
    "bow_prediction = lr_bow.predict(bow_features)\n",
    "tfidf_prediction = lr_tfidf.predict(tfidf_features)\n",
    "\n",
    "print(\"Prediction (BOW):\", bow_prediction)\n",
    "print(\"Prediction (TF-IDF):\", tfidf_prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
